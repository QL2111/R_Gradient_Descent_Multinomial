% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/LogisticRegressionMultinomial.R
\name{LogisticRegressionMultinomial}
\alias{LogisticRegressionMultinomial}
\title{Logistic Regression Multinomial R6 Class}
\usage{
LogisticRegressionMultinomial$new(learning_rate = 0.01, num_iterations = 1000, loss = "logistique", optimizer = "adam", beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8, patience = 20, use_early_stopping = TRUE, regularization = "none", batch_size = 32)

LogisticRegressionMultinomial$new(learning_rate = 0.01, num_iterations = 1000, loss = "logistique", optimizer = "adam", beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8, patience = 20, use_early_stopping = TRUE, regularization = "none", batch_size = 32)

fit(X, y, validation_split = 0.2)

adam_optimizer(X_batch, y_batch, m, v, beta1, beta2, learning_rate, epsilon, i, coefficients)

sgd_optimizer(X_batch, y_batch, learning_rate, coefficients)

validate(X_val, y_val, unique_classes)

softmax(z)

one_hot_encode(y, unique_classes)

predict(X)

var_importance()

plot_loss()

print(X_test, y_test)

log_loss(y_true, y_pred)

mse_loss(y_true, y_pred)

predict_proba(X)

select_variables(num_variables)

apply_regularization(gradient, coefficients, p = 0.5)

export_pmml(file_path)
}
\description{
The `LogisticRegressionMultinomial` class implements multinomial logistic regression using gradient descent and the Adam optimizer.
}
\details{
This R6 class supports multiple loss functions, optimizers, regularization methods, and early stopping based on validation loss.
}
\examples{
\dontrun{
model <- LogisticRegressionMultinomial$new(learning_rate = 0.01, num_iterations = 1000, optimizer = "adam")
model$fit(X, y, validation_split = 0.2)
}

## ------------------------------------------------
## Method `LogisticRegressionMultinomial$new`
## ------------------------------------------------

\dontrun{
model <- LogisticRegressionMultinomial$new(learning_rate = 0.01, num_iterations = 1000, optimizer = "adam")
}

## ------------------------------------------------
## Method `LogisticRegressionMultinomial$fit`
## ------------------------------------------------

\dontrun{
model <- LogisticRegressionMultinomial$new()
model$fit(X, y, validation_split = 0.2)
}
Adam Optimizer for Multinomial Logistic Regression

This function performs a single update of the coefficients using the Adam optimization algorithm. It computes the loss, gradients, and updates the coefficients based on the first and second moment estimates.

## ------------------------------------------------
## Method `LogisticRegressionMultinomial$adam_optimizer`
## ------------------------------------------------

\dontrun{
result <- adam_optimizer(X_batch, y_batch, m, v, beta1, beta2, learning_rate, epsilon, i, coefficients)
}

## ------------------------------------------------
## Method `LogisticRegressionMultinomial$sgd_optimizer`
## ------------------------------------------------

\dontrun{
coefficients <- sgd_optimizer(X_batch, y_batch, learning_rate, coefficients)
}

## ------------------------------------------------
## Method `LogisticRegressionMultinomial$validate`
## ------------------------------------------------

# Assuming `model` is an instance of the logistic regression model
loss <- model$validate(X_val, y_val, unique_classes)

## ------------------------------------------------
## Method `LogisticRegressionMultinomial$softmax`
## ------------------------------------------------

# Assuming `z` is a matrix of linear model outputs
probabilities <- softmax(z)

## ------------------------------------------------
## Method `LogisticRegressionMultinomial$one_hot_encode`
## ------------------------------------------------

# Assuming `y` is a vector of class labels
one_hot_y <- one_hot_encode(y, unique_classes)

## ------------------------------------------------
## Method `LogisticRegressionMultinomial$predict`
## ------------------------------------------------

# Assuming `model` is an instance of the logistic regression model
predictions <- model$predict(X_test)

## ------------------------------------------------
## Method `LogisticRegressionMultinomial$plot_loss`
## ------------------------------------------------

\dontrun{
model$plot_loss()
}

## ------------------------------------------------
## Method `LogisticRegressionMultinomial$plot_auc`
## ------------------------------------------------

\dontrun{
model$plot_auc(X_test, y_test)
}

## ------------------------------------------------
## Method `LogisticRegressionMultinomial$summary`
## ------------------------------------------------

\dontrun{
model$summary()
}

## ------------------------------------------------
## Method `LogisticRegressionMultinomial$print`
## ------------------------------------------------

\dontrun{
model$print(X_test, y_test)
}


## ------------------------------------------------
## Method `LogisticRegressionMultinomial$log_loss`
## ------------------------------------------------

# Example usage:
y_true <- c(1, 0, 1)  # True labels (one-hot encoded for binary classification)
y_pred <- c(0.9, 0.2, 0.8)  # Predicted probabilities
log_loss(y_true, y_pred)


## ------------------------------------------------
## Method `LogisticRegressionMultinomial$mse_loss`
## ------------------------------------------------

y_true <- c(1, 0, 1, 0)
y_pred <- c(0.9, 0.1, 0.8, 0.2)
mse_loss(y_true, y_pred)

## ------------------------------------------------
## Method `LogisticRegressionMultinomial$predict_proba`
## ------------------------------------------------

# Assuming `model` is an instance of the logistic regression model
probabilities <- model$predict_proba(X_test)

## ------------------------------------------------
## Method `LogisticRegressionMultinomial$select_variables`
## ------------------------------------------------

\dontrun{
model$select_variables(num_variables = 5)
}

## ------------------------------------------------
## Method `LogisticRegressionMultinomial$apply_regularization`
## ------------------------------------------------

\dontrun{
result <- apply_regularization(gradient, coefficients, p = 0.5)
}

## ------------------------------------------------
## Method `LogisticRegressionMultinomial$export_pmml`
## ------------------------------------------------

\dontrun{
model$export_pmml("logistic_regression_multinomial.pmml")
}
}
\section{Public fields}{
\if{html}{\out{<div class="r6-fields">}}
\describe{
\item{\code{coefficients}}{Matrix of model coefficients(weights), initialized during the `fit` method.}

\item{\code{learning_rate}}{Numeric. Learning rate for gradient descent optimization. Default is 0.01.}

\item{\code{num_iterations}}{Integer. Number of iterations for gradient descent optimization. Default is 1000.}

\item{\code{loss_history}}{Numeric vector. Tracks the loss at each iteration during training.}

\item{\code{beta1}}{Numeric. Momentum parameter for Adam optimizer. Default is 0.9.}

\item{\code{beta2}}{Numeric. Second momentum parameter for Adam optimizer. Default is 0.999.}

\item{\code{epsilon}}{Numeric. Small constant for numerical stability in Adam optimizer. Default is 1e-8.}

\item{\code{use_early_stopping}}{Logical. Whether to use early stopping based on validation loss. Default is TRUE.}

\item{\code{patience}}{Integer. Number of iterations to wait for improvement before stopping early. Default is 20.}

\item{\code{regularization}}{Character. Regularization method to use. Options are "none", "ridge", "lasso", "elasticnet". Default is "none".}

\item{\code{loss_function}}{Function. Loss function to use for optimization. Options are "quadratique", "logistique". Default is "logistique".}

\item{\code{loss_name}}{Character. Name of the loss function used.}

\item{\code{optimizer}}{Character. Optimizer to use for gradient descent. Options are "adam", "sgd". Default is "adam".}

\item{\code{batch_size}}{Integer. Size of the mini-batch for gradient descent. Default is 32.}
}
\if{html}{\out{</div>}}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-LogisticRegressionMultinomial-new}{\code{LogisticRegressionMultinomial$new()}}
\item \href{#method-LogisticRegressionMultinomial-fit}{\code{LogisticRegressionMultinomial$fit()}}
\item \href{#method-LogisticRegressionMultinomial-adam_optimizer}{\code{LogisticRegressionMultinomial$adam_optimizer()}}
\item \href{#method-LogisticRegressionMultinomial-sgd_optimizer}{\code{LogisticRegressionMultinomial$sgd_optimizer()}}
\item \href{#method-LogisticRegressionMultinomial-validate}{\code{LogisticRegressionMultinomial$validate()}}
\item \href{#method-LogisticRegressionMultinomial-softmax}{\code{LogisticRegressionMultinomial$softmax()}}
\item \href{#method-LogisticRegressionMultinomial-one_hot_encode}{\code{LogisticRegressionMultinomial$one_hot_encode()}}
\item \href{#method-LogisticRegressionMultinomial-predict}{\code{LogisticRegressionMultinomial$predict()}}
\item \href{#method-LogisticRegressionMultinomial-var_importance}{\code{LogisticRegressionMultinomial$var_importance()}}
\item \href{#method-LogisticRegressionMultinomial-plot_loss}{\code{LogisticRegressionMultinomial$plot_loss()}}
\item \href{#method-LogisticRegressionMultinomial-plot_auc}{\code{LogisticRegressionMultinomial$plot_auc()}}
\item \href{#method-LogisticRegressionMultinomial-summary}{\code{LogisticRegressionMultinomial$summary()}}
\item \href{#method-LogisticRegressionMultinomial-print}{\code{LogisticRegressionMultinomial$print()}}
\item \href{#method-LogisticRegressionMultinomial-log_loss}{\code{LogisticRegressionMultinomial$log_loss()}}
\item \href{#method-LogisticRegressionMultinomial-mse_loss}{\code{LogisticRegressionMultinomial$mse_loss()}}
\item \href{#method-LogisticRegressionMultinomial-predict_proba}{\code{LogisticRegressionMultinomial$predict_proba()}}
\item \href{#method-LogisticRegressionMultinomial-select_variables}{\code{LogisticRegressionMultinomial$select_variables()}}
\item \href{#method-LogisticRegressionMultinomial-apply_regularization}{\code{LogisticRegressionMultinomial$apply_regularization()}}
\item \href{#method-LogisticRegressionMultinomial-export_pmml}{\code{LogisticRegressionMultinomial$export_pmml()}}
\item \href{#method-LogisticRegressionMultinomial-clone}{\code{LogisticRegressionMultinomial$clone()}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-LogisticRegressionMultinomial-new"></a>}}
\if{latex}{\out{\hypertarget{method-LogisticRegressionMultinomial-new}{}}}
\subsection{Method \code{new()}}{
Initializes a new instance of the `LogisticRegressionMultinomial` class.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{LogisticRegressionMultinomial$new(
  learning_rate = 0.01,
  num_iterations = 1000,
  loss = "logistique",
  optimizer = "adam",
  beta1 = 0.9,
  beta2 = 0.999,
  epsilon = 1e-08,
  patience = 20,
  use_early_stopping = TRUE,
  regularization = "none",
  batch_size = 32
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{learning_rate}}{Numeric. Sets the learning rate for gradient descent. Default is 0.01.}

\item{\code{num_iterations}}{Integer. Specifies the number of gradient descent iterations. Default is 1000.}

\item{\code{loss}}{Character. Specifies the loss function to use. Options are "logistique", "quadratique", "deviance". Default is "logistique".}

\item{\code{optimizer}}{Character. Specifies the optimizer to use. Options are "adam", "sgd". Default is "adam".}

\item{\code{beta1}}{Numeric. Momentum parameter for Adam optimizer. Default is 0.9.}

\item{\code{beta2}}{Numeric. Second momentum parameter for Adam optimizer. Default is 0.999.}

\item{\code{epsilon}}{Numeric. Small constant for numerical stability in Adam optimizer. Default is 1e-8.}

\item{\code{patience}}{Integer. Number of iterations to wait for improvement before stopping early. Default is 10.}

\item{\code{use_early_stopping}}{Logical. Whether to use early stopping. Default is TRUE.}

\item{\code{regularization}}{Character. Regularization method to use. Options are "none", "ridge", "lasso", "elasticnet". Default is "none".}

\item{\code{batch_size}}{Integer. Size of the mini-batch for gradient descent. Default is 32, put 1 for online learning.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
A new `LogisticRegressionMultinomial` object.
Fit the Multinomial Logistic Regression Model
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{\dontrun{
model <- LogisticRegressionMultinomial$new(learning_rate = 0.01, num_iterations = 1000, optimizer = "adam")
}
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-LogisticRegressionMultinomial-fit"></a>}}
\if{latex}{\out{\hypertarget{method-LogisticRegressionMultinomial-fit}{}}}
\subsection{Method \code{fit()}}{
This function fits a multinomial logistic regression model to the given data using either the Adam or SGD optimizer.
By default, the model uses early stopping based on the validation loss with a patience of 20 iterations.
It will also use by default the cross logisitc loss function, the Adam optimizer with a mini-batch of 32, and no regularization.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{LogisticRegressionMultinomial$fit(X, y, validation_split = 0.2)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{X}}{A matrix or data frame of input features.}

\item{\code{y}}{A factor vector of target labels.}

\item{\code{validation_split}}{A numeric value indicating the proportion of the data to be used for validation (default is 0.2).}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
None. The function updates the model's coefficients in place.
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{\dontrun{
model <- LogisticRegressionMultinomial$new()
model$fit(X, y, validation_split = 0.2)
}
Adam Optimizer for Multinomial Logistic Regression

This function performs a single update of the coefficients using the Adam optimization algorithm. It computes the loss, gradients, and updates the coefficients based on the first and second moment estimates.
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-LogisticRegressionMultinomial-adam_optimizer"></a>}}
\if{latex}{\out{\hypertarget{method-LogisticRegressionMultinomial-adam_optimizer}{}}}
\subsection{Method \code{adam_optimizer()}}{
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{LogisticRegressionMultinomial$adam_optimizer(
  X_batch,
  y_batch,
  m,
  v,
  beta1,
  beta2,
  learning_rate,
  epsilon,
  i,
  coefficients
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{X_batch}}{A matrix of input features for the current batch.}

\item{\code{y_batch}}{A factor vector of response variables for the current batch.}

\item{\code{m}}{A matrix of the first moment estimates.}

\item{\code{v}}{A matrix of the second moment estimates.}

\item{\code{beta1}}{The exponential decay rate for the first moment estimates.}

\item{\code{beta2}}{The exponential decay rate for the second moment estimates.}

\item{\code{learning_rate}}{The learning rate for the optimizer.}

\item{\code{epsilon}}{A small constant for numerical stability.}

\item{\code{i}}{The current iteration number.}

\item{\code{coefficients}}{A matrix of current coefficients.}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
The Adam optimizer updates the coefficients using the following formulas:
\deqn{m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t}
\deqn{v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2}
\deqn{\hat{m}_t = \frac{m_t}{1 - \beta_1^t}}
\deqn{\hat{v}_t = \frac{v_t}{1 - \beta_2^t}}
\deqn{\theta_t = \theta_{t-1} - \alpha \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}}
where \eqn{m_t} and \eqn{v_t} are the first and second moment estimates, \eqn{\beta_1} and \eqn{\beta_2} are the exponential decay rates, \eqn{\alpha} is the learning rate, and \eqn{\epsilon} is a small constant for numerical stability.
}

\subsection{Returns}{
A list containing the updated coefficients, first moment estimates (m), second moment estimates (v), and the loss value.
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{\dontrun{
result <- adam_optimizer(X_batch, y_batch, m, v, beta1, beta2, learning_rate, epsilon, i, coefficients)
}
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-LogisticRegressionMultinomial-sgd_optimizer"></a>}}
\if{latex}{\out{\hypertarget{method-LogisticRegressionMultinomial-sgd_optimizer}{}}}
\subsection{Method \code{sgd_optimizer()}}{
This function performs a single step of Stochastic Gradient Descent (SGD) optimization for multinomial logistic regression.
It computes the linear model, probabilities, error, and gradients for the current batch. It then updates the coefficients using the gradients and the learning rate.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{LogisticRegressionMultinomial$sgd_optimizer(
  X_batch,
  y_batch,
  learning_rate,
  coefficients
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{X_batch}}{A matrix of input features for the current batch.}

\item{\code{y_batch}}{A factor vector of target labels for the current batch.}

\item{\code{learning_rate}}{A numeric value representing the learning rate for SGD.}

\item{\code{coefficients}}{A matrix of current coefficients for the logistic regression model.}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
The sgd optimizer first converts the target labels `y_batch` into a one-hot encoded matrix. It then computes the linear model as:
\deqn{Z = X_{batch} \cdot \beta}
where \eqn{X_{batch}} is the input feature matrix and \eqn{\beta} are the coefficients.

The probabilities are computed using the softmax function:
\deqn{P = \text{softmax}(Z)}

The error is calculated as the difference between the predicted probabilities and the one-hot encoded target labels:
\deqn{\text{error} = P - \text{one\_hot\_y}}

The gradient of the loss with respect to the coefficients is computed as:
\deqn{\nabla L = \frac{1}{N} X_{batch}^T \cdot \text{error}}
where \eqn{N} is the number of samples in the batch.

Finally, the coefficients are updated using the gradient and the learning rate:
\deqn{\beta = \beta - \text{learning\_rate} \cdot \nabla L}
}

\subsection{Returns}{
A matrix of updated coefficients after performing one step of SGD.
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{\dontrun{
coefficients <- sgd_optimizer(X_batch, y_batch, learning_rate, coefficients)
}
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-LogisticRegressionMultinomial-validate"></a>}}
\if{latex}{\out{\hypertarget{method-LogisticRegressionMultinomial-validate}{}}}
\subsection{Method \code{validate()}}{
This function validates the model using the provided validation data.
It computes the class probabilities for the validation data and calculates the log loss using the one-hot encoded target labels.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{LogisticRegressionMultinomial$validate(X_val, y_val, unique_classes)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{X_val}}{A matrix of validation features.}

\item{\code{y_val}}{A vector of validation labels.}

\item{\code{unique_classes}}{A vector of unique class labels.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
The log loss of the validation data.
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{# Assuming `model` is an instance of the logistic regression model
loss <- model$validate(X_val, y_val, unique_classes)
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-LogisticRegressionMultinomial-softmax"></a>}}
\if{latex}{\out{\hypertarget{method-LogisticRegressionMultinomial-softmax}{}}}
\subsection{Method \code{softmax()}}{
Computes the softmax of the input matrix. The softmax function is used to convert the linear model outputs into class probabilities, projecting them into the range [0, 1].
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{LogisticRegressionMultinomial$softmax(z)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{z}}{A matrix of linear model outputs.}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
The softmax function is defined as:
\deqn{softmax(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}}
where \(z_i\) is the \(i\)-th element of the input matrix \(z\).
}

\subsection{Returns}{
A matrix of softmax probabilities for each class.
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{# Assuming `z` is a matrix of linear model outputs
probabilities <- softmax(z)
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-LogisticRegressionMultinomial-one_hot_encode"></a>}}
\if{latex}{\out{\hypertarget{method-LogisticRegressionMultinomial-one_hot_encode}{}}}
\subsection{Method \code{one_hot_encode()}}{
One-hot encodes the response variable, converting it into a binary matrix. Each row corresponds to a sample, and each column corresponds to a class label.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{LogisticRegressionMultinomial$one_hot_encode(y, unique_classes)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{y}}{A vector representing the response variable.}

\item{\code{unique_classes}}{A vector of unique class labels.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
A binary matrix where each row corresponds to a sample, and each column corresponds to a class.
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{# Assuming `y` is a vector of class labels
one_hot_y <- one_hot_encode(y, unique_classes)
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-LogisticRegressionMultinomial-predict"></a>}}
\if{latex}{\out{\hypertarget{method-LogisticRegressionMultinomial-predict}{}}}
\subsection{Method \code{predict()}}{
Predicts the class labels for new data. The function calculates logits and converts them into class probabilities using the softmax function.
It then returns the class with the highest probability for each sample.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{LogisticRegressionMultinomial$predict(X)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{X}}{A data frame or matrix of predictors, where rows are samples and columns are features.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
A vector of predicted class labels for each sample.
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{# Assuming `model` is an instance of the logistic regression model
predictions <- model$predict(X_test)
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-LogisticRegressionMultinomial-var_importance"></a>}}
\if{latex}{\out{\hypertarget{method-LogisticRegressionMultinomial-var_importance}{}}}
\subsection{Method \code{var_importance()}}{
This function calculates the importance of each feature based on the absolute value of the coefficients.
It averages the absolute coefficients across all classes and sorts them in descending order. It then prints the variable importance scores and plots a bar chart to visualize the importance of each feature.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{LogisticRegressionMultinomial$var_importance()}\if{html}{\out{</div>}}
}

\subsection{Returns}{
A bar plot showing the variable importance scores.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-LogisticRegressionMultinomial-plot_loss"></a>}}
\if{latex}{\out{\hypertarget{method-LogisticRegressionMultinomial-plot_loss}{}}}
\subsection{Method \code{plot_loss()}}{
This function plots the loss history to visualize the convergence of the loss function over iterations.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{LogisticRegressionMultinomial$plot_loss()}\if{html}{\out{</div>}}
}

\subsection{Details}{
The plot_loss function checks if the loss history is available and non-empty. If the loss history is empty, it stops and prompts the user to run the 'fit' method first. Otherwise, it plots the loss history.
}

\subsection{Returns}{
A plot showing the convergence of the loss function over iterations.
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{\dontrun{
model$plot_loss()
}
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-LogisticRegressionMultinomial-plot_auc"></a>}}
\if{latex}{\out{\hypertarget{method-LogisticRegressionMultinomial-plot_auc}{}}}
\subsection{Method \code{plot_auc()}}{
This function calculates and plots the ROC AUC for the model predictions. It uses the One vs All strategy to calculate the ROC AUC for each class and plots the ROC curve for each class.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{LogisticRegressionMultinomial$plot_auc(X_test, y_test, probabilities = NULL)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{X_test}}{A data frame or matrix containing the test features.}

\item{\code{y_test}}{A vector containing the true labels for the test data.}

\item{\code{probabilities}}{A matrix of class probabilities for the test data. If not provided, the model will predict probabilities using the test features.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
A plot showing the ROC curve and the AUC value.
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{\dontrun{
model$plot_auc(X_test, y_test)
}
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-LogisticRegressionMultinomial-summary"></a>}}
\if{latex}{\out{\hypertarget{method-LogisticRegressionMultinomial-summary}{}}}
\subsection{Method \code{summary()}}{
Displays the hyperparameters of the trained model.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{LogisticRegressionMultinomial$summary()}\if{html}{\out{</div>}}
}

\subsection{Details}{
The summary function prints out the hyperparameters of the model, including the optimizer, learning rate, number of iterations, and loss function. If the optimizer is "adam", it also prints out the Adam-specific parameters: Beta1, Beta2, and Epsilon.
It will also print out the regularization method, batch size, and early stopping parameters.
}

\subsection{Returns}{
None. This function is used for its side effect of printing the model's hyperparameters.
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{\dontrun{
model$summary()
}
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-LogisticRegressionMultinomial-print"></a>}}
\if{latex}{\out{\hypertarget{method-LogisticRegressionMultinomial-print}{}}}
\subsection{Method \code{print()}}{
This function prints the results of the logistic regression multinomial model on the test data.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{LogisticRegressionMultinomial$print(X_test, y_test)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{X_test}}{A data frame or matrix containing the test features.}

\item{\code{y_test}}{A vector containing the true labels for the test data.}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
The print function performs the following steps:
\itemize{
  \item Predicts the labels for the test data using the model.
  \item Computes and prints the confusion matrix.
  \item Computes and prints the classification report using the `caret` package.
  \item Computes and prints the weighted F1 score using the `MLmetrics` package.
}
}

\subsection{Returns}{
This function does not return a value. It prints the confusion matrix, classification report, and F1 score.
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{\dontrun{
model$print(X_test, y_test)
}

}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-LogisticRegressionMultinomial-log_loss"></a>}}
\if{latex}{\out{\hypertarget{method-LogisticRegressionMultinomial-log_loss}{}}}
\subsection{Method \code{log_loss()}}{
This function computes the log loss (logarithmic loss) for a given set of true 
labels and predicted probabilities, with an optional regularization penalty.
The log loss is a common metric for evaluating classification models, especially
in logistic regression and neural networks.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{LogisticRegressionMultinomial$log_loss(y_true, y_pred)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{y_true}}{A numeric vector or matrix of true class labels in one-hot encoding 
format (i.e., a binary indicator for each class).}

\item{\code{y_pred}}{A numeric vector or matrix of predicted probabilities for each class. 
Values must be between 0 and 1.}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
The log loss is computed using the formula:
\deqn{J = - \sum_{i=1}^{n} \sum_{k=1}^{K} y_{ik} \times \log(\pi_{ik})}
where:
\itemize{
  \item \( y_{ik} \) is the true label for observation \( i \) and class \( k \) (0 or 1),
  \item \( \pi_{ik} \) is the predicted probability for observation \( i \) and class \( k \).
}
A small value (\code{epsilon = 1e-15}) is used to avoid the computation of \code{log(0)}.

If regularization is enabled, the penalty is added to the loss:
\deqn{\text{loss} = \text{log loss} + \text{regularization penalty}}
}

\subsection{Returns}{
The computed log loss value with an added regularization penalty, if applicable.
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{# Example usage:
y_true <- c(1, 0, 1)  # True labels (one-hot encoded for binary classification)
y_pred <- c(0.9, 0.2, 0.8)  # Predicted probabilities
log_loss(y_true, y_pred)

}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-LogisticRegressionMultinomial-mse_loss"></a>}}
\if{latex}{\out{\hypertarget{method-LogisticRegressionMultinomial-mse_loss}{}}}
\subsection{Method \code{mse_loss()}}{
This function computes the mean squared error (MSE) loss between the true labels 
and the predicted values.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{LogisticRegressionMultinomial$mse_loss(y_true, y_pred)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{y_true}}{A numeric vector of true labels.}

\item{\code{y_pred}}{A numeric vector of predicted values.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
A numeric value representing the MSE loss.
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{y_true <- c(1, 0, 1, 0)
y_pred <- c(0.9, 0.1, 0.8, 0.2)
mse_loss(y_true, y_pred)
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-LogisticRegressionMultinomial-predict_proba"></a>}}
\if{latex}{\out{\hypertarget{method-LogisticRegressionMultinomial-predict_proba}{}}}
\subsection{Method \code{predict_proba()}}{
Predicts the class probabilities for new data.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{LogisticRegressionMultinomial$predict_proba(X)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{X}}{A data frame or matrix of predictors, where rows are samples and columns are features.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
A matrix of predicted class probabilities for each sample.
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{# Assuming `model` is an instance of the logistic regression model
probabilities <- model$predict_proba(X_test)
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-LogisticRegressionMultinomial-select_variables"></a>}}
\if{latex}{\out{\hypertarget{method-LogisticRegressionMultinomial-select_variables}{}}}
\subsection{Method \code{select_variables()}}{
Displays the selected variables based on their importance scores.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{LogisticRegressionMultinomial$select_variables(num_variables)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{num_variables}}{An integer specifying the number of top variables to display.}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
The select variables function calculates the importance of each feature based on the absolute value of the coefficients. It sums the absolute coefficients for each feature and selects the top 'num_variables' features based on their importance scores.
}

\subsection{Returns}{
None. This function is used for its side effect of printing the selected variables.
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{\dontrun{
model$select_variables(num_variables = 5)
}
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-LogisticRegressionMultinomial-apply_regularization"></a>}}
\if{latex}{\out{\hypertarget{method-LogisticRegressionMultinomial-apply_regularization}{}}}
\subsection{Method \code{apply_regularization()}}{
Applies regularization to the gradient and computes the penalty term for the loss function.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{LogisticRegressionMultinomial$apply_regularization(
  gradient,
  coefficients,
  p = 0.5
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{gradient}}{A matrix of gradients with respect to the model coefficients.}

\item{\code{coefficients}}{A matrix of model coefficients, where the first row corresponds to the intercept.}

\item{\code{p}}{A numeric value (default 0.5) representing the mixing parameter for ElasticNet regularization.}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
The ElasticNet regularization combines L1 and L2 penalties. The penalty term is computed as:
\deqn{penalty = \frac{\lambda}{2} \left( (1 - p) \sum_{j=1}^{n} \beta_j^2 + p \sum_{j=1}^{n} |\beta_j| \right)}
where \(\lambda\) is the regularization parameter, \(p\) is the mixing parameter, \(\beta_j\) are the model coefficients, and \(n\) is the number of coefficients.

For Ridge regularization, the penalty term is computed as:
\deqn{penalty = \frac{1}{2} \sum_{j=1}^{n} \beta_j^2}
where \(\beta_j\) are the model coefficients.

For Lasso regularization, the penalty term is computed as:
\deqn{penalty = \frac{1}{2} \sum_{j=1}^{n} |\beta_j|}
where \(\beta_j\) are the model coefficients.
}

\subsection{Returns}{
A list containing:
  - `penalty`: The computed penalty term to be added to the loss function.
  - `regularized_gradient`: The gradient matrix adjusted for regularization.
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{\dontrun{
result <- apply_regularization(gradient, coefficients, p = 0.5)
}
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-LogisticRegressionMultinomial-export_pmml"></a>}}
\if{latex}{\out{\hypertarget{method-LogisticRegressionMultinomial-export_pmml}{}}}
\subsection{Method \code{export_pmml()}}{
Exports the trained model to a PMML (Predictive Model Markup Language) file.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{LogisticRegressionMultinomial$export_pmml(file_path)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{file_path}}{A string specifying the path where the PMML file will be saved.}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
The export_pml function generates a PMML file for a multinomial logistic regression model, including the model's
  coefficients and metadata. It ensures that the model is trained before exporting and uses the PMML version 4.4 format.
}

\subsection{Returns}{
Saves the PMML representation of the trained model to the specified file and returns a success message.
}
\subsection{Examples}{
\if{html}{\out{<div class="r example copy">}}
\preformatted{\dontrun{
model$export_pmml("logistic_regression_multinomial.pmml")
}
}
\if{html}{\out{</div>}}

}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-LogisticRegressionMultinomial-clone"></a>}}
\if{latex}{\out{\hypertarget{method-LogisticRegressionMultinomial-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{LogisticRegressionMultinomial$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
