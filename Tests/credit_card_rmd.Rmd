```{r}

# Charger les bibliothèques nécessaires
library(R6)
# install.packages("nnet")
library(nnet)
# Charger les fichiers de fonctions
source("R/DataPreparer.R")
source("R/factor_analysis_mixed.R")
source("R/LogisticRegressionMultinomial.R")

print("Credit Card Approval Prediction Example")

# Charger le jeu de données depuis un fichier local après téléchargement de Kaggle
data_path <- "data/credit_card.csv"  # Remplacez par le chemin de votre fichier
data <- read.csv(data_path)

# S'assurer que la variable cible est un facteur
data$Industry <- as.factor(data$Industry)

# Diviser les données en ensembles d'entraînement et de test
set.seed(42)  # Pour la reproductibilité

```

```{r}
train_indices <- sample(1:nrow(data), size = 0.7 * nrow(data))  # 70% pour l'entraînement
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]

# Séparer les caractéristiques et la variable cible
X_train <- train_data[, -which(names(train_data) == "Approved")] # On essaye avec Approved au lieu d'Industry
y_train <- train_data$Industry

X_test <- test_data[, -which(names(test_data) == "Approved")]
y_test <- test_data$Industry
```

```{r}
# Afficher les dimensions des ensembles d'entraînement et de test avant le traitement
dim(X_train)
dim(X_test)
# Préparer les prédicteurs sans inclure la variable cible
data_prep <- DataPreparer$new(use_factor_analysis = FALSE)  # standardisation + one hot encoding
prepared_X_train <- data_prep$prepare_data(X_train)
# Préparer les mêmes transformation aux données de test
prepared_X_test <- data_prep$prepare_data(X_test)

# Enlever les duplicates
cat("Number of duplicates in prepared training data:", sum(duplicated(prepared_X_train)), "\n")
cat("Number of duplicates in prepared test data:", sum(duplicated(prepared_X_test)), "\n")
prepared_X_train <- prepared_X_train[!duplicated(prepared_X_train), ]
prepared_X_test <- prepared_X_test[!duplicated(prepared_X_test), ]
cat("Number of duplicates in prepared training data after removal:", sum(duplicated(prepared_X_train)), "\n")
cat("Number of duplicates in prepared test data after removal:", sum(duplicated(prepared_X_test)), "\n")

# Afficher les dimensions des données préparées
dim(prepared_X_train)
dim(prepared_X_test)

# Afficher la proportion de valeurs manquantes dans les données préparées
cat("Proportion of missing values in prepared training data:", mean(is.na(prepared_X_train)), "\n")
cat("Proportion of missing values in prepared test data:", mean(is.na(prepared_X_test)), "\n")

# Removing missing values
prepared_train_data = na.omit(prepared_X_train)
prepared_test_data = na.omit(prepared_X_test)

cat("Proportion of missing values in prepared training data after preparation:", mean(is.na(prepared_X_train)), "\n")
cat("Proportion of missing values in prepared test data: after preparation", mean(is.na(prepared_X_test)), "\n")

```

```{r}
# Convertir les données préparées en matrices
X_train_matrix <- as.matrix(prepared_X_train)
X_test_matrix <- as.matrix(prepared_X_test)

# Convertir la variable cible en valeurs numériques
y_train_numeric <- as.numeric(y_train)
y_test_numeric <- as.numeric(y_test)
```

```{r}
# Initialiser et ajuster le modèle sur l'ensemble d'entraînement
model <- LogisticRegressionMultinomial$new(learning_rate = 0.1, num_iterations = 1000)
model$fit(X_train_matrix, y_train_numeric)
print("Model trained successfully")
```
```{r}
predictions <- model$predict(X_test_matrix)
print(X_test_matrix)
# Vérifier si des prédictions ont été faites

```
```{r}
print(y_test_numeric)
# Calculer et afficher l'accuracy
accuracy <- sum(predictions == y_test_numeric) / length(y_test_numeric)
cat("Accuracy:", accuracy, "\n")
```
```{r}
dim(predictions)
dim(y_test_numeric)
confusion_matrix <- table(Predicted = predictions, Actual = y_test_numeric)
print(confusion_matrix)

```
```{r}

```
Je veux maintenant comparé avec un autre model importer pour voir si le problème vient de mon model ou de la préparation des données
```{r}
# 
# Initialiser le modèle de régression logistique multinomial
multinom_model <- nnet::multinom(y_train_numeric ~ ., data = as.data.frame(X_train_matrix))

# Faire des prédictions sur l'ensemble de test
multinom_predictions <- predict(multinom_model, newdata = as.data.frame(X_test_matrix))

# Vérifier les prédictions
print("Multinomial Model Predictions:")
print(multinom_predictions)

# Calculer et afficher l'accuracy
multinom_accuracy <- sum(multinom_predictions == y_test_numeric) / length(y_test_numeric)
cat("Multinomial Model Accuracy:", multinom_accuracy, "\n")

# Calculer et afficher la matrice de confusion
multinom_confusion_matrix <- table(Predicted = multinom_predictions, Actual = y_test_numeric)
print("Multinomial Model Confusion Matrix:")
print(multinom_confusion_matrix)
```
Ce modèle importer de nnet affiche 32% d'accuracy, ce qui est assez faible mais meilleur que notre 9 %, donc le problème vient de notre model.
On va essayer avec une autre variable cible, plus facile à prédire.
Avec la variable "Industry", on a 89% d'accuracy avec le model importer et 5% avec notre modèle, donc le problème vient de notre modèle.